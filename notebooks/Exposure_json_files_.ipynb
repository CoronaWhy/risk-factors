{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Exposure- json files .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonpolishko/colab-notebooks-sink/blob/master/task-risk/Exposure_json_files_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByDGQtsz3mtW",
        "colab_type": "text"
      },
      "source": [
        "task-risk\n",
        "https://trello.com/c/lGTIDqwq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_USq_DTBgho",
        "colab_type": "text"
      },
      "source": [
        "# Importing Key Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptFn63qqBghr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01QjldBsBghx",
        "colab_type": "text"
      },
      "source": [
        "# Reading in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGipnonFBghy",
        "colab_type": "code",
        "outputId": "3dc53757-3b25-4e30-ae05-017fb5e854b5",
        "colab": {}
      },
      "source": [
        "os.chdir('C:\\\\Users\\\\Cafral\\\\Desktop\\\\kaggle\\\\CORD-19-research-challenge\\\\2020-03-13\\\\') #change this to your relevnt directory\n",
        "root_path = 'C:\\\\Users\\\\Cafral\\\\Desktop\\\\kaggle\\\\CORD-19-research-challenge\\\\2020-03-13'  #change this to your relevnt directory\n",
        "all_json_paths = glob.glob(f'{root_path}\\\\**\\\\*.json', recursive=True)\n",
        "len(all_json_paths)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13202"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRrcR3p6Bgh5",
        "colab_type": "code",
        "outputId": "21210341-e78f-4701-a45f-7ece34070b93",
        "colab": {}
      },
      "source": [
        "#source:https://www.kaggle.com/amogh05/cord-19-eda-question-topic-modeling-starter\n",
        "#add more vars as required\n",
        "\n",
        "class FileReader:\n",
        "    def __init__(self, file_path):\n",
        "        with open(file_path) as file:\n",
        "            content = json.load(file)\n",
        "            self.paper_id = content['paper_id']\n",
        "            self.title = content['metadata']['title']\n",
        "            self.abstract = []\n",
        "            self.body_text = []\n",
        "            self.biblio = []\n",
        "            self.biblio_doi = []\n",
        "            self.img_tables = []\n",
        "            self.back_matter = []\n",
        "            \n",
        "            \n",
        "            # Abstract\n",
        "            for entry in content['abstract']:\n",
        "                self.abstract.append(entry['text'])\n",
        "            self.abstract = '\\n'.join(self.abstract)\n",
        "            \n",
        "            # Body text\n",
        "            for entry in content['body_text']:\n",
        "                self.body_text.append(entry['text'])          \n",
        "            self.body_text = '\\n'.join(self.body_text)\n",
        "            \n",
        "            # bibliography\n",
        "            for bib_id, details in content['bib_entries'].items():\n",
        "                self.biblio.append(details['title'])\n",
        "                self.biblio_doi.append(details['other_ids'])\n",
        "            self.biblio = '\\n'.join(self.biblio)\n",
        "            #self.biblio_doi = '\\n'.join(self.biblio_doi)\n",
        "            \n",
        "            #img and table references\n",
        "            for ref_id,details in content['ref_entries'].items():\n",
        "                self.img_tables.append(details['text'])\n",
        "            self.img_tables = '\\n'.join(self.img_tables)\n",
        "            \n",
        "            #back_matter\n",
        "            for entry in content['back_matter']:\n",
        "                self.back_matter.append(entry['text'])\n",
        "            self.back_matter = '\\n'.join(self.back_matter)\n",
        "            \n",
        "    def __repr__(self):\n",
        "        return f'{self.paper_id}:{self.title}-{self.abstract}... {self.body_text}...{self.biblio}...{self.img_tables}...{self.back_matter}'\n",
        "        \n",
        "    \n",
        "dict_ = {'paper_id': [],'title':[], 'abstract': [], 'body_text': [],'biblio':[],'bidoi':[],'img_tables':[]}\n",
        "for idx, entry in enumerate(all_json_paths):\n",
        "    if idx % (len(all_json_paths) // 10) == 0:\n",
        "        print(f'Processing index: {idx} of {len(all_json_paths)}')\n",
        "    #print(entry)\n",
        "    content = FileReader(entry)\n",
        "    dict_['paper_id'].append(content.paper_id)\n",
        "    dict_['title'].append(content.title)\n",
        "    dict_['abstract'].append(content.abstract)\n",
        "    dict_['body_text'].append(content.body_text)\n",
        "    dict_['biblio'].append(content.biblio)   \n",
        "    dict_['img_tables'].append(content.img_tables)  \n",
        "df_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text','biblio','img_tables'])\n",
        "df_covid.head()\n",
        "\n",
        "#identify dups\n",
        "df_covid.describe(include='all')\n",
        "\n",
        "df_covid.drop_duplicates(['abstract'], inplace=True)\n",
        "df_covid.describe(include='all')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing index: 0 of 13202\n",
            "Processing index: 1320 of 13202\n",
            "Processing index: 2640 of 13202\n",
            "Processing index: 3960 of 13202\n",
            "Processing index: 5280 of 13202\n",
            "Processing index: 6600 of 13202\n",
            "Processing index: 7920 of 13202\n",
            "Processing index: 9240 of 13202\n",
            "Processing index: 10560 of 13202\n",
            "Processing index: 11880 of 13202\n",
            "Processing index: 13200 of 13202\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_id</th>\n",
              "      <th>abstract</th>\n",
              "      <th>body_text</th>\n",
              "      <th>biblio</th>\n",
              "      <th>img_tables</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>count</td>\n",
              "      <td>11113</td>\n",
              "      <td>11113</td>\n",
              "      <td>11113</td>\n",
              "      <td>11113</td>\n",
              "      <td>11113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>unique</td>\n",
              "      <td>11113</td>\n",
              "      <td>11113</td>\n",
              "      <td>11113</td>\n",
              "      <td>11110</td>\n",
              "      <td>10788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>top</td>\n",
              "      <td>7e6fc218bd723f70d305eadff23e036875076250</td>\n",
              "      <td>Infection of Lewis rats with the murine corona...</td>\n",
              "      <td>Infection &amp; Immunity Medical Investigation Uni...</td>\n",
              "      <td>Ebola by the numbers: The size, spread and cos...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>freq</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>326</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        paper_id  \\\n",
              "count                                      11113   \n",
              "unique                                     11113   \n",
              "top     7e6fc218bd723f70d305eadff23e036875076250   \n",
              "freq                                           1   \n",
              "\n",
              "                                                 abstract  \\\n",
              "count                                               11113   \n",
              "unique                                              11113   \n",
              "top     Infection of Lewis rats with the murine corona...   \n",
              "freq                                                    1   \n",
              "\n",
              "                                                body_text  \\\n",
              "count                                               11113   \n",
              "unique                                              11113   \n",
              "top     Infection & Immunity Medical Investigation Uni...   \n",
              "freq                                                    1   \n",
              "\n",
              "                                                   biblio img_tables  \n",
              "count                                               11113      11113  \n",
              "unique                                              11110      10788  \n",
              "top     Ebola by the numbers: The size, spread and cos...             \n",
              "freq                                                    2        326  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4g3odpnBgh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_covid['all_text'] = df_covid['abstract'] + '' + df_covid['body_text'] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzSi43YiBgh_",
        "colab_type": "text"
      },
      "source": [
        "# Find synonyms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP4LiZK6Bgh_",
        "colab_type": "code",
        "outputId": "354c606d-1375-4ee6-b9ab-d50c77026cb1",
        "colab": {}
      },
      "source": [
        "#This approach does not  work well:  defining list mannually better\n",
        "import nltk \n",
        "from nltk.corpus import wordnet \n",
        "synonyms = [] \n",
        "\n",
        "  \n",
        "for syn in wordnet.synsets('exposure'): \n",
        "    for l in syn.lemmas(): \n",
        "        synonyms.append(l.name()) \n",
        "        if l.antonyms(): \n",
        "            antonyms.append(l.antonyms()[0].name())\n",
        "print(set(synonyms))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'picture', 'photo', 'photograph', 'exposure', 'pic', 'vulnerability'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgcCgM0dBgiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining list mannually better\n",
        "stage_syn_list = ['exposure','vulnerability','vulnerable'] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HeiIWjBBgiF",
        "colab_type": "text"
      },
      "source": [
        "# Filter By Stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psF8crebBgiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "disease_stage_list = ['exposure' ,'acquisition' ,'progression', 'development' ,'complications' ,'fatality', 'disability']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "799CUu-QBgiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filterByStage(text,stage_syn_list):\n",
        "    paper_list =[]\n",
        "    \n",
        "    for idx_num,row in text.iterrows():\n",
        "        for stage in stage_syn_list:\n",
        "            stage_found = False\n",
        "            if stage in row.all_text.split():\n",
        "                stage_found = True\n",
        "            else:\n",
        "                pass \n",
        "        if stage_found==True:\n",
        "            paper_list.append(row.all_text)\n",
        "    return paper_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f--Oywl2BgiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stage_dict = {}\n",
        "\n",
        "stage = disease_stage_list[0]\n",
        "\n",
        "stage_dict[stage] = filterByStage(df_covid,stage_syn_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqBwy-cBBgiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for later ease while searching for relevant papers\n",
        "exposure = pd.DataFrame(stage_dict[stage])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVZ7kb-lBgiR",
        "colab_type": "text"
      },
      "source": [
        "# NLP Starts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SW7BY4EBgiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('C:\\\\Users\\\\Cafral\\\\Desktop\\\\kaggle\\\\en_core_sci_lg-0.2.4\\\\en_core_sci_lg-0.2.4\\\\en_core_sci_lg\\\\en_core_sci_lg-0.2.4')\n",
        "\n",
        "# We also need to detect language, or else we'll be parsing non-english text \n",
        "# as if it were English. \n",
        "from spacy_langdetect import LanguageDetector\n",
        "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
        "\n",
        "nlp.max_length=2000000\n",
        "\n",
        "# New stop words list \n",
        "customize_stop_words = [\n",
        "    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n",
        "    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'fig', 'fig.', 'al.',\n",
        "    'di', 'la', 'il', 'del', 'le', 'della', 'dei', 'delle', 'una', 'da',  'dell',  'non', 'si'\n",
        "]\n",
        "\n",
        "# Mark them as stop words\n",
        "for w in customize_stop_words:\n",
        "    nlp.vocab[w].is_stop = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxBNAcnfBgiV",
        "colab_type": "text"
      },
      "source": [
        "# LDA : Kaggle Notebook Approach\n",
        "https://www.kaggle.com/danielwolffram/topic-modeling-finding-related-articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_9eTyZUBgiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spacy_tokenizer(sentence):\n",
        "    return [word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)] \n",
        "    # remove numbers (e.g. from references [1], etc.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHowpLdvBgiY",
        "colab_type": "text"
      },
      "source": [
        "### Create vector representation of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67rnpT5yBgiY",
        "colab_type": "code",
        "outputId": "e82fb073-be2d-43e4-dec6-bbb4c05f7f2b",
        "colab": {
          "referenced_widgets": [
            "669f984b48d5431cb5bbb3a1260c9dff"
          ]
        }
      },
      "source": [
        "tf_vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, max_features=800000) \n",
        "tf = tf_vectorizer.fit_transform(tqdm(stage_dict[stage]))\n",
        "\n",
        "print(tf.shape)\n",
        "\n",
        "import joblib\n",
        "joblib.dump(tf_vectorizer, 'tf_vectorizer.csv')\n",
        "joblib.dump(tf, 'tf.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "669f984b48d5431cb5bbb3a1260c9dff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2992.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-15-ff0f309f3fe5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtf_vectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy_tokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstage_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstage\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1058\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    968\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    971\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    350\u001b[0m                                                tokenize)\n\u001b[0;32m    351\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 352\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-14-64397bedc8b8>\u001b[0m in \u001b[0;36mspacy_tokenizer\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mspacy_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlike_num\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stop\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_punct\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_space\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;31m# remove numbers (e.g. from references [1], etc.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    437\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__call__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tagger.__call__\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tagger.set_annotations\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.extend_tensor\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\util.py\u001b[0m in \u001b[0;36mcopy_array\u001b[1;34m(dst, src, casting, where)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcopy_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"same_kind\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[0mdst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_cupy_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWEc66wABgib",
        "colab_type": "text"
      },
      "source": [
        "### LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE6W07IBBgib",
        "colab_type": "code",
        "outputId": "8daeaa9c-8636-4f04-c173-2c1092779388",
        "colab": {}
      },
      "source": [
        "lda_tf = LatentDirichletAllocation(n_components=10, random_state=0)\n",
        "lda_tf.fit(tf)\n",
        "joblib.dump(lda_tf, 'lda.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
              "                          evaluate_every=-1, learning_decay=0.7,\n",
              "                          learning_method='batch', learning_offset=10.0,\n",
              "                          max_doc_update_iter=100, max_iter=10,\n",
              "                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n",
              "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
              "                          total_samples=1000000.0, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSNlxmPABgii",
        "colab_type": "text"
      },
      "source": [
        "### Discover Topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh6roGudBgij",
        "colab_type": "code",
        "outputId": "b7d437f1-4186-4ad9-d7b5-0fc901e6f26c",
        "colab": {}
      },
      "source": [
        "tfidf_feature_names = tf_vectorizer.get_feature_names()\n",
        "\n",
        "def print_top_words(model, vectorizer, n_top_words):\n",
        "    feature_names = vectorizer.get_feature_names()\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        message = \"\\nTopic #%d: \" % topic_idx\n",
        "        message += \" \".join([feature_names[i]\n",
        "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        print(message)\n",
        "    print()\n",
        "    \n",
        "print_top_words(lda_tf, tfidf_feature_names, 25)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Topic #0: virus infection sample study human viral animal disease respiratory bat specie test case detect high mers-cov sequence pathogen host include report antibody positive strain result\n",
            "\n",
            "Topic #1: case disease health study datum model infection risk outbreak number transmission time contact report population include individual estimate high rate control influenza public result increase\n",
            "\n",
            "Topic #2: cell infection mouse response expression virus immune viral gene level increase study lung show cytokine induce control effect result activation disease day type macrophage production\n",
            "\n",
            "Topic #3: cat pedv group study effect increase treatment level gene cell calf cancer pig high disease animal piglet expression result tissue stress control compare liver show\n",
            "\n",
            "Topic #4: cell virus protein viral membrane rna fusion particle show mm structure infection result replication domain contain entry activity mutant ph residue indicate mutation observe min\n",
            "\n",
            "Topic #5: antibody cell protein vaccine activity target binding show virus human assay study bind drug high result epitope acid antigen mab dna peptide different strain site\n",
            "\n",
            "Topic #6: un che con sono nel alla ha più gli tra degli salute ad ed come stato stati nella rischio studio ai nei hanno anche stata\n",
            "\n",
            "Topic #7: patient study group day result increase high clinical treatment icu respiratory level associate mortality include blood hospital lung severe compare pneumonia method acute score outcome\n",
            "\n",
            "Topic #8: protein cell gene virus show sequence host viral analysis study infection expression result pathway membrane dna peptide domain rna acid human response autophagy identify genome\n",
            "\n",
            "Topic #9: cell mouse culture show protein antibody study expression result control increase min medium human incubate tissue membrane level wash time mm observe datum contain surface\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iyid7cJBgip",
        "colab_type": "text"
      },
      "source": [
        "### Create Topic Distance Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrLbqxiyBgiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topic_dist = pd.DataFrame(lda_tf.transform(tf))\n",
        "topic_dist.to_csv('topic_dist.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaTZa7l4Bgit",
        "colab_type": "code",
        "outputId": "4548c53f-90c7-442a-f1be-b26443fc9fc5",
        "colab": {}
      },
      "source": [
        "topic_dist.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.011124</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.027063</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000838</td>\n",
              "      <td>0.437342</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.010219</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.449822</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.222896</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.647718</td>\n",
              "      <td>0.008317</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.029781</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.285344</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.041184</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 50 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2         3         4         5         6   \\\n",
              "0  0.000015  0.000015  0.000015  0.000015  0.000015  0.000015  0.000015   \n",
              "1  0.000005  0.000005  0.000005  0.000005  0.000005  0.000005  0.000005   \n",
              "2  0.000010  0.000010  0.000010  0.000010  0.000010  0.000010  0.000010   \n",
              "3  0.000009  0.000009  0.000009  0.000009  0.000009  0.222896  0.000009   \n",
              "4  0.029781  0.000035  0.000035  0.000035  0.000035  0.000035  0.000035   \n",
              "\n",
              "         7         8         9   ...        40        41        42        43  \\\n",
              "0  0.000015  0.000015  0.000015  ...  0.000015  0.000015  0.011124  0.000015   \n",
              "1  0.000005  0.000005  0.000005  ...  0.000005  0.027063  0.000005  0.000838   \n",
              "2  0.000010  0.000010  0.000010  ...  0.000010  0.000010  0.010219  0.000010   \n",
              "3  0.000009  0.647718  0.008317  ...  0.000009  0.000009  0.000009  0.000009   \n",
              "4  0.000035  0.000035  0.000035  ...  0.000035  0.000035  0.000035  0.285344   \n",
              "\n",
              "         44        45        46        47        48        49  \n",
              "0  0.000015  0.000015  0.000015  0.000015  0.000015  0.000015  \n",
              "1  0.437342  0.000005  0.000005  0.000005  0.000005  0.000005  \n",
              "2  0.449822  0.000010  0.000010  0.000010  0.000010  0.000010  \n",
              "3  0.000009  0.000009  0.000009  0.000009  0.000009  0.000009  \n",
              "4  0.000035  0.000035  0.000035  0.000035  0.000035  0.041184  \n",
              "\n",
              "[5 rows x 50 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3yT53kaBgiv",
        "colab_type": "text"
      },
      "source": [
        "### Get Paper Related to Stage of Disease"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TayqD0SBgiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Need synonyms especially here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Iw5VbE1Bgiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get most similar paper\n",
        "from scipy.spatial import distance\n",
        "def get_k_nearest_docs(doc_dist, k=5, lower=1950, upper=2020, only_covid19=False, get_dist=False):\n",
        "    '''\n",
        "    doc_dist: topic distribution (sums to 1) of one article\n",
        "    \n",
        "    Returns the index of the k nearest articles (as by Jensen–Shannon divergence in topic space). \n",
        "    '''\n",
        "    \n",
        "    #relevant_time = df.publish_year.between(lower, upper)\n",
        "    \n",
        "   # if only_covid19:\n",
        "   #     is_covid19_article = df.body_text.str.contains('COVID-19|SARS-CoV-2|2019-nCov|SARS Coronavirus 2|2019 Novel Coronavirus') #TODO: move outside\n",
        "   #     topic_dist_temp = topic_dist[relevant_time & is_covid19_article]\n",
        "   #     \n",
        "   # else:\n",
        "    #    topic_dist_temp = topic_dist[relevant_time]\n",
        "    \n",
        "    distances = topic_dist.apply(lambda x: distance.jensenshannon(x, doc_dist), axis=1)\n",
        "    k_nearest = distances[distances != 0].nsmallest(n=k).index\n",
        "    \n",
        "    if get_dist:\n",
        "        k_distances = distances[distances != 0].nsmallest(n=k)\n",
        "        return k_nearest, k_distances\n",
        "    else:\n",
        "        return k_nearest\n",
        "    \n",
        "#d = get_k_nearest_docs(topic_dist[1].iloc[0],k=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58uvnzDPBgiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relevant_articles(df,tasks, k=3, lower=1950, upper=2020, only_covid19=False):\n",
        "    tasks = [tasks] if type(tasks) is str else tasks \n",
        "    \n",
        "    tasks_tf = tf_vectorizer.transform(tasks)\n",
        "    tasks_topic_dist = pd.DataFrame(lda_tf.transform(tasks_tf))\n",
        "\n",
        "    for index, bullet in enumerate(tasks):\n",
        "        print(bullet)\n",
        "        recommended = get_k_nearest_docs(tasks_topic_dist.iloc[index], k, lower, upper, only_covid19)\n",
        "        print(list(recommended))\n",
        "        recommended = df.iloc[recommended] #stage_dict[stage][','.join(list(recommended))]#\n",
        "    return recommended"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtIX0_IEBgi2",
        "colab_type": "code",
        "outputId": "feb00cff-a1c4-4111-c506-f159f83ca091",
        "colab": {}
      },
      "source": [
        "task = ['exposure']\n",
        "relevant_articles(exposure,task,k=3) #k is the number of relevant articles"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "exposure\n",
            "[475, 406, 1450]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>The International Consortium for Prevention an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>406</td>\n",
              "      <td>Aerobiology plays a fundamental role in the tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>Background: Because of the high global prevale...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      0\n",
              "475   The International Consortium for Prevention an...\n",
              "406   Aerobiology plays a fundamental role in the tr...\n",
              "1450  Background: Because of the high global prevale..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    }
  ]
}